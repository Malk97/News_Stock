{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForSequenceClassification, AutoTokenizer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "model_path = 'G:/Malk/Qafza/Final_Project/models/Model'\n",
    "model = AutoModelForSequenceClassification.from_pretrained(model_path)\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_path)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data preorocess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to C:\\Users\\Aya/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to C:\\Users\\Aya/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     C:\\Users\\Aya/nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n",
      "[nltk_data] Downloading package wordnet to C:\\Users\\Aya/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "from deep_translator import GoogleTranslator\n",
    "from nltk.corpus import stopwords, wordnet\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from collections import defaultdict\n",
    "from unidecode import unidecode \n",
    "from collections import Counter\n",
    "from langdetect import detect\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "import nltk\n",
    "import re\n",
    "\n",
    "# Download necessary NLTK resources\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "nltk.download('wordnet')\n",
    "\n",
    "# Load dataset\n",
    "df = pd.read_csv(\"G:\\\\Malk\\\\Qafza\\\\Final_Project\\\\data\\\\news.csv\")\n",
    "\n",
    "# Initialize tools\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "# Additional custom stopwords (optional)\n",
    "custom_stopwords = {\n",
    "    \"will\", \"new\", \"york\", \"years\", \"one\", \"said\", \"united\", \"state\", \"trump\", \"time\",\n",
    "    \"country\", \"two\", \"city\", \"china\", \"first\", \"woman\", \"american\", \"make\", \"made\",\n",
    "    \"work\", \"company\", \"take\", \"family\", \"president\", \"government\", \"plan\", \"life\",\n",
    "    \"people\", \"say\", \"says\", \"saying\", \"may\", \"show\", \"look\", \"help\", \"many\", \"home\",\n",
    "    \"year\", \"day\", \"even\", \"women\", \"team\", \"teams\", \"states\", \"child\", \"russia\", \"would\",\n",
    "    \"part\", \"world\", \"want\", \"set\", \"way\", \"found\", \"group\", \"played\", \"playing\", \"time\", \"election\", \"charge\",\n",
    "    \"player\", \"play\", \"countries\", \"country\", \"plays\", \"become\", \"becomes\", \"became\", \"right\",\n",
    "    \"three\", \"come\", \"needing\", \"came\", \"comes\", \"weeks\", \"week\", \"need\", \"needed\",\n",
    "    \"needs\", \"official\", \"still\", \"including\", \"former\", \"last\", \"party\", \"star\",\n",
    "    \"back\", \"place\", \"change\", \"return\", \"leader\", \"offer\", \"history\", \"season\",\n",
    "    \"support\", \"couple\", \"met\", \"know\", \"find\", \"hope\", \"others\", \"power\", \"game\",\n",
    "    \"talk\", \"toke\", \"token\", \"call\", \"called\", \"calling\", \"calls\", \"million\", \"can\",\n",
    "    \"give\", \"given\", \"giving\", \"gives\", \"go\", \"going\", \"gone\", \"goes\", \"could\", \"get\", \"also\", \"open\",\n",
    "    \"take\", \"taken\", \"taking\", \"takes\", \"bring\", \"bringing\", \"brought\", \"brings\",\n",
    "    \"old\", \"run\", \"running\", \"ran\", \"runs\", \"use\", \"used\", \"using\", \"uses\",\n",
    "    \"try\", \"trying\", \"tried\", \"tries\", \"artist\", \"business\", \"police\", \"report\", \"protest\", \"case\", \"start\", \"started\",\n",
    "    \"starts\", \"end\", \"ending\", \"ended\", \"much\", \"big\", \"large\", \"top\", \"official\", \"case\", \"month\", \"plan\", \"appear\", \"live\", \"long\", \"man\",\n",
    "    \"move\", \"moves\", \"moved\", \"moving\", \"tell\", \"tells\", \"told\", \"telling\", \"face\", \"faces\", \"faced\", \"facing\",\n",
    "    \"show\", \"shows\", \"showed\", \"showing\", \"know\", \"knows\", \"knew\", \"knowing\", \"offer\", \"offers\", \"offered\", \"offering\",\n",
    "    \"begin\", \"begins\", \"began\", \"beginning\", \"hold\", \"holds\", \"held\", \"holding\", \"put\", \"puts\", \"putting\", \"took\",\n",
    "    \"bring\", \"brings\", \"brought\", \"bringing\", \"call\", \"calls\", \"called\", \"calling\", \"run\", \"runs\", \"ran\", \"running\", \"use\", \"uses\", \"used\", \"using\",\n",
    "    \"try\", \"tries\", \"tried\", \"trying\", \"see\", \"sees\", \"saw\", \"seeing\", \"seen\", \"seening\"\n",
    "}\n",
    "all_stopwords = stop_words.union(custom_stopwords)\n",
    "\n",
    "# Function to normalize text (lowercase, remove special characters)\n",
    "def normalize_text(text):\n",
    "    text = text.lower().strip() \n",
    "    text = unidecode(text) \n",
    "    text = re.sub(r'[^a-z\\s]', ' ', text) \n",
    "    text = re.sub(r'\\s+', ' ', text)\n",
    "    return text\n",
    "\n",
    "# Function to map POS tags for better lemmatization\n",
    "def get_wordnet_pos_bulk(words):\n",
    "    tag_dict = defaultdict(lambda: wordnet.NOUN, {\"J\": wordnet.ADJ, \"V\": wordnet.VERB, \"R\": wordnet.ADV})\n",
    "    return [(word, tag_dict.get(pos[0].upper(), wordnet.NOUN)) for word, pos in nltk.pos_tag(words)]\n",
    "\n",
    "# Main preprocessing function\n",
    "def preprocess_text(text):\n",
    "    if not isinstance(text, str) or len(text) < 10: \n",
    "        return None  \n",
    "    text = normalize_text(text)  # Normalize text\n",
    "\n",
    "    # Tokenize and remove stopwords\n",
    "    words = [word for word in word_tokenize(text) if word not in all_stopwords and len(word) > 2]\n",
    "    \n",
    "    # POS tagging and lemmatization\n",
    "    tagged_words = get_wordnet_pos_bulk(words)\n",
    "    cleaned_words = [lemmatizer.lemmatize(word, pos) for word, pos in tagged_words]\n",
    "\n",
    "    return ' '.join(cleaned_words) if cleaned_words else None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted class: Positive\n"
     ]
    }
   ],
   "source": [
    "new_text = \"The stock market is expected to rise tomorrow.\"\n",
    "new_text=preprocess_text(new_text)\n",
    "inputs = tokenizer(new_text, return_tensors='pt')\n",
    "outputs = model(**inputs)\n",
    "predictions = outputs.logits.argmax(dim=-1)\n",
    "print(\"Predicted class:\", \"Positive\" if predictions.item() == 1 else \"Negative\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted class: Positive\n"
     ]
    }
   ],
   "source": [
    "new_text = \"The government announced new economic policies to stimulate growth.\"\n",
    "inputs = tokenizer(new_text, return_tensors='pt')\n",
    "outputs = model(**inputs)\n",
    "predictions = outputs.logits.argmax(dim=-1)\n",
    "print(\"Predicted class:\", \"Positive\" if predictions.item() == 1 else \"Negative\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted class: Negative\n"
     ]
    }
   ],
   "source": [
    "new_text = \"The stock market is facing a significant downturn due to the economic crisis.\"\n",
    "inputs = tokenizer(new_text, return_tensors='pt')\n",
    "outputs = model(**inputs)\n",
    "predictions = outputs.logits.argmax(dim=-1)\n",
    "print(\"Predicted class:\", \"Positive\" if predictions.item() == 1 else \"Negative\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted class: Negative\n"
     ]
    }
   ],
   "source": [
    "new_text = \"The company reported disappointing earnings, leading to a sharp decline in its stock price.\"\n",
    "inputs = tokenizer(new_text, return_tensors='pt')\n",
    "outputs = model(**inputs)\n",
    "predictions = outputs.logits.argmax(dim=-1)\n",
    "print(\"Predicted class:\", \"Positive\" if predictions.item() == 1 else \"Negative\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted class: Negative\n"
     ]
    }
   ],
   "source": [
    "new_text = \"The recent market volatility has caused investors to panic and sell their stocks.\"\n",
    "inputs = tokenizer(new_text, return_tensors='pt')\n",
    "outputs = model(**inputs)\n",
    "predictions = outputs.logits.argmax(dim=-1)\n",
    "print(\"Predicted class:\", \"Positive\" if predictions.item() == 1 else \"Negative\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Ranking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Title: Trump: New travel barriers for Canadian tourists, the biggest source of US tourism. Expect impact on airlines, hotels, retail, restaurants\n",
      "Upvotes: 2746\n",
      "Number of Comments: 487\n",
      "-------------------------------------------------- 1\n",
      "Title: EU Targets €26 Billion of US Products in Tariff Retaliation\n",
      "Upvotes: 198\n",
      "Number of Comments: 25\n",
      "-------------------------------------------------- 2\n",
      "Title: The problem with TSLA stock\n",
      "Upvotes: 0\n",
      "Number of Comments: 28\n",
      "-------------------------------------------------- 3\n",
      "Title: These are the stocks on my watchlist (03/12)\n",
      "Upvotes: 10\n",
      "Number of Comments: 7\n",
      "-------------------------------------------------- 4\n",
      "Title: CPiI increased by 0.2% MoM and the annual rate of increase was 2.8% in Feb\n",
      "Upvotes: 15\n",
      "Number of Comments: 5\n",
      "-------------------------------------------------- 5\n"
     ]
    }
   ],
   "source": [
    "import praw\n",
    "\n",
    "client_id = 'yu7KW2fhRmQBUW3LDL1X2A'\n",
    "client_secret = 'VZa78pB0H3bVlwOWsnsgV3yzFBLkSw'\n",
    "user_agent = 'praw:com.example.myapp:v1.0 (by /u/Sad-Information9604)'\n",
    "\n",
    "reddit = praw.Reddit(client_id=client_id, client_secret=client_secret, user_agent=user_agent, redirect_uri='http://localhost:8000')\n",
    "\n",
    "# البحث عن المشاركات المتعلقة بالأسواق المالية\n",
    "subreddit = reddit.subreddit('stocks')  # يمكنك تخصيص هذا إلى subreddits معينة مثل 'stocks', 'finance', إلخ\n",
    "query = 'The stock market is facing a significant downturn due to the economic crisis.'\n",
    "\n",
    "# عدد المشاركات التي تريد جلبها\n",
    "num_posts = 10\n",
    "ne=subreddit.search(query, limit=num_posts,time_filter='day')\n",
    "# البحث عن المشاركات\n",
    "number_of_post=0\n",
    "Total=0\n",
    "for submission in ne:\n",
    "    number_of_post+=1\n",
    "    Total +=submission.score+submission.num_comments\n",
    "    print(f\"Title: {submission.title}\")\n",
    "    print(f\"Upvotes: {submission.score}\")\n",
    "    print(f\"Number of Comments: {submission.num_comments}\")\n",
    "    print(\"-\" * 50,number_of_post)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.2125523012552301\n"
     ]
    }
   ],
   "source": [
    "import praw\n",
    "\n",
    "def get_reddit_posts(client_id, client_secret, user_agent, subreddit_name, query, num_posts=20, time_filter='day'):\n",
    "    \n",
    "    reddit = praw.Reddit(client_id=client_id, client_secret=client_secret, user_agent=user_agent, redirect_uri='http://localhost:8000')\n",
    "\n",
    "    subreddit = reddit.subreddit(subreddit_name)  \n",
    "    posts = subreddit.search(query, limit=num_posts, time_filter=time_filter)\n",
    "    \n",
    "    post_data = []\n",
    "    number_of_post = 0\n",
    "    total = 0\n",
    "\n",
    "    for submission in posts:\n",
    "        number_of_post += 1\n",
    "        score_plus_comments = submission.score + submission.num_comments\n",
    "        post_data.append(score_plus_comments)\n",
    "        total += score_plus_comments\n",
    "\n",
    "    ratio = number_of_post / total if total != 0 else 0  \n",
    "    \n",
    "    normalized_values = []\n",
    "    if post_data:  \n",
    "        min_value = min(post_data)\n",
    "        max_value = max(post_data)\n",
    "\n",
    "        for value in post_data:\n",
    "            if max_value != min_value:\n",
    "                normalized_value = (value - min_value) / (max_value - min_value)\n",
    "            else:\n",
    "                normalized_value = 0  \n",
    "            normalized_values.append(normalized_value)\n",
    "        \n",
    "    return sum(normalized_values)/number_of_post\n",
    "\n",
    "\n",
    "query = 'The stock market is facing a significant downturn due to the economic crisis.'\n",
    "\n",
    "normalized_values = get_reddit_posts(client_id, client_secret, user_agent, subreddit_name='stocks', query=query, num_posts=20, time_filter='day')\n",
    "\n",
    "print(normalized_values)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "float"
      ]
     },
     "execution_count": 104,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(normalized_values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.21245482346399774\n"
     ]
    }
   ],
   "source": [
    "import praw\n",
    "import os\n",
    "\n",
    "def get_reddit_posts(query):\n",
    "    \n",
    "    reddit = praw.Reddit(client_id='yu7KW2fhRmQBUW3LDL1X2A',\n",
    "                         client_secret='VZa78pB0H3bVlwOWsnsgV3yzFBLkSw',\n",
    "                         user_agent='praw:com.example.myapp:v1.0 (by /u/Sad-Information9604)', \n",
    "                         redirect_uri='http://localhost:8000')\n",
    "\n",
    "    subreddit = reddit.subreddit('stocks')  \n",
    "    posts = subreddit.search(query, limit=50, time_filter='day')\n",
    "    \n",
    "    post_data = []\n",
    "    number_of_post = 0\n",
    "    total = 0\n",
    "\n",
    "    for submission in posts:\n",
    "        number_of_post += 1\n",
    "        score_plus_comments = submission.score + submission.num_comments\n",
    "        post_data.append(score_plus_comments)\n",
    "        total += score_plus_comments\n",
    "\n",
    "    ratio = number_of_post / total if total != 0 else 0  \n",
    "    \n",
    "    normalized_values = []\n",
    "    if post_data:  \n",
    "        min_value = min(post_data)\n",
    "        max_value = max(post_data)\n",
    "\n",
    "        for value in post_data:\n",
    "            if max_value != min_value:\n",
    "                normalized_value = (value - min_value) / (max_value - min_value)\n",
    "            else:\n",
    "                normalized_value = 0  \n",
    "            normalized_values.append(normalized_value)\n",
    "        \n",
    "    return sum(normalized_values)/number_of_post\n",
    "\n",
    "\n",
    "\n",
    "def ranking(text, sentiment_score, source_credibility):\n",
    "    alpha, beta, gamma = 0.5, 0.3, 0.2\n",
    "    rank_score = alpha * sentiment_score + beta * get_reddit_posts(text) + gamma * source_credibility\n",
    "    return rank_score\n",
    "\n",
    "\n",
    "query = 'The stock market is facing a significant downturn due to the economic crisis.'\n",
    "normalized_values = get_reddit_posts(query=query)\n",
    "print(normalized_values)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cpu\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'label': 'Negative', 'score': 0.9966173768043518}, {'label': 'Positive', 'score': 1.0}, {'label': 'Negative', 'score': 0.9999710321426392}, {'label': 'Neutral', 'score': 0.9889442920684814}]\n"
     ]
    }
   ],
   "source": [
    "from transformers import BertTokenizer, BertForSequenceClassification\n",
    "from transformers import pipeline\n",
    "\n",
    "finbert = BertForSequenceClassification.from_pretrained('yiyanghkust/finbert-tone',num_labels=3)\n",
    "tokenizer = BertTokenizer.from_pretrained('yiyanghkust/finbert-tone')\n",
    "\n",
    "nlp = pipeline(\"sentiment-analysis\", model=finbert, tokenizer=tokenizer)\n",
    "\n",
    "sentences = [\"there is a shortage of capital, and we need extra financing\",  \n",
    "             \"growth is strong and we have plenty of liquidity\", \n",
    "             \"there are doubts about our finances\", \n",
    "             \"profits are flat\"]\n",
    "results = nlp(sentences)\n",
    "print(results)  #LABEL_0: neutral; LABEL_1: positive; LABEL_2: negative"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
